{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes Python version 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import several Python packages to check availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib\n",
    "import scipy\n",
    "import bokeh\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These were added explicitly to environment.yml for binder.\n",
    "import autocorrect\n",
    "import plotly\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download some data to use with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  The nltk.download() function with no argument launches a dialog to choose data such as corpora, but you can also provide an argument to specify what to download. See: https://stackoverflow.com/questions/5843817/programmatically-install-nltk-corpora-models-i-e-without-the-gui-downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/peckhams/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package verbnet to\n",
      "[nltk_data]     /Users/peckhams/nltk_data...\n",
      "[nltk_data]   Package verbnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('verbnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some tests with WordNet from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boil.n.01\n",
      "boiling_point.n.01\n",
      "boil.v.01\n",
      "boil.v.02\n",
      "boil.v.03\n",
      "churn.v.02\n",
      "seethe.v.02\n",
      " \n",
      "jump.n.01\n",
      "leap.n.02\n",
      "jump.n.03\n",
      "startle.n.01\n",
      "jump.n.05\n",
      "jump.n.06\n",
      "jump.v.01\n",
      "startle.v.02\n",
      "jump.v.03\n",
      "jump.v.04\n",
      "leap_out.v.01\n",
      "jump.v.06\n",
      "rise.v.11\n",
      "jump.v.08\n",
      "derail.v.02\n",
      "chute.v.01\n",
      "jump.v.11\n",
      "jumpstart.v.01\n",
      "jump.v.13\n",
      "leap.v.02\n",
      "alternate.v.01\n",
      " \n",
      "flow.n.01\n",
      "flow.n.02\n",
      "flow.n.03\n",
      "flow.n.04\n",
      "stream.n.04\n",
      "stream.n.02\n",
      "menstruation.n.01\n",
      "flow.v.01\n",
      "run.v.06\n",
      "flow.v.03\n",
      "flow.v.04\n",
      "hang.v.05\n",
      "flow.v.06\n",
      "menstruate.v.01\n",
      " \n",
      "gyrate.v.01\n",
      "spin.v.01\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "sets = wn.synsets('boil')\n",
    "for s in sets:\n",
    "    print( s.name() )\n",
    "    # print( s.pos() )\n",
    "\n",
    "print(' ')\n",
    "sets = wn.synsets('jump')\n",
    "for s in sets:\n",
    "    print( s.name() )\n",
    "    # print( s.pos() )\n",
    "\n",
    "print(' ')\n",
    "sets = wn.synsets('flow')\n",
    "for s in sets:\n",
    "    print( s.name() )\n",
    "    # print( s.pos() )\n",
    "    \n",
    "print(' ')\n",
    "sets = wn.synsets('gyrate')\n",
    "for s in sets:\n",
    "    print( s.name() )\n",
    "    # print( s.pos() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some tests with VerbNet from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'December',\n",
       " u'FedEx',\n",
       " u'UPS',\n",
       " u'abandon',\n",
       " u'abase',\n",
       " u'abash',\n",
       " u'abate',\n",
       " u'abbreviate',\n",
       " u'abduct',\n",
       " u'abet',\n",
       " u'abhor',\n",
       " u'abolish',\n",
       " u'abound',\n",
       " u'abrade',\n",
       " u'abridge',\n",
       " u'absolve',\n",
       " u'abstain',\n",
       " u'abstract',\n",
       " u'abuse',\n",
       " u'abut',\n",
       " u'accelerate',\n",
       " u'accept',\n",
       " u'acclaim',\n",
       " u'accompany',\n",
       " u'accrue']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import verbnet as vn\n",
    "vn.lemmas()[0:25]\n",
    "\n",
    "# help(vn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment for generating nominalizations of verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GSN process names are nominalizations of verbs.  One idea for how to automatically generate a list of these process names from a list of verbs (e.g. from VerbNet) is to concatenate some of the 11 or so possible verb nominalization endings directly to the verb, then apply autocorrect and check if the result is a noun.  Standard endings are:\n",
    "**tion** (absorption, convection),\n",
    "**sion** (conversion, dispersion),\n",
    "**cion** (suspicion, coercion),\n",
    "**ing** (swimming, upwelling),\n",
    "**age** (drainage, seepage),\n",
    "__y__ (discovery, recovery),\n",
    "**al** (arrival, retrieval),\n",
    "**ance** (acceptance, attendance),\n",
    "**ence** (existence, maintanence)\n",
    "**ment** (alignment, improvement),\n",
    "**ure** (failure, departure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['proposition', 'proposal', 'proposeence']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autocorrect import spell\n",
    "\n",
    "[spell('proposetion'), spell('proposeal'), spell('proposeence')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that **spell** may return strings that aren't words with no change, like the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['distraction', u'distracted', 'distractence']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[spell('distracttion'),spell('distractal'),spell('distractence')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wailment', 'faience', 'family', 'failure']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[spell('failment'),spell('failence'),spell('faily'),spell('failure')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get root verb of a nominalized verb using a PPattach algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import verbnet as vn\n",
    "\n",
    "def get_root_verb( noun ):\n",
    "    verbs = list()\n",
    "    for sense in wn.synsets(noun, pos=wn.NOUN):\n",
    "        for lemma in sense.lemmas():\n",
    "            name = str(lemma.name())   # remove the \"u\"\n",
    "            # print( [noun, name] )\n",
    "            # if (name[0:3] == noun[0:3]):   ######\n",
    "            if (name == noun):\n",
    "                forms = lemma.derivationally_related_forms()\n",
    "                for form in forms:\n",
    "                    # form is of type Lemma\n",
    "                    fset = form.synset()\n",
    "                    if (fset.pos() == wn.VERB):\n",
    "                        verb = form.name()\n",
    "                        verbs.append( verb )\n",
    "\n",
    "    verbs = sorted( set(verbs) )\n",
    "    n_verbs = len(verbs)\n",
    "    if (n_verbs == 0):\n",
    "        print('### Sorry, no root verb found for: ' + noun)\n",
    "    else:\n",
    "        # print( 'n_verbs = ', n_verbs )\n",
    "        dum = 0\n",
    "    for verb in verbs:\n",
    "        print( verb )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abate\n",
      "#### Sorry, no root verb found for: abseiling\n",
      "acidify\n",
      "accrue\n",
      "#### Sorry, no root verb found for: canyoning\n",
      "#### Sorry, no root verb found for: caving\n",
      "clear\n",
      "close\n",
      "closure\n",
      "decide\n",
      "diagnose\n",
      "distribute\n",
      "drain\n",
      "electrify\n",
      "evaporate\n",
      "fail\n",
      "form\n",
      "hypnotise\n",
      "hypnotize\n",
      "impact\n",
      "infiltrate\n",
      "interrogate\n",
      "maintain\n",
      "#### Sorry, no root verb found for: macgyvering\n",
      "melt\n",
      "#### Sorry, no root verb found for: picnicking\n",
      "propose\n",
      "propose\n",
      "proposition\n",
      "#### Sorry, no root verb found for: rafting\n",
      "rebel\n",
      "reconcile\n",
      "retrieve\n",
      "segment\n",
      "#### Sorry, no root verb found for: spelunking\n",
      "swim\n"
     ]
    }
   ],
   "source": [
    "processes = ['abatement','abseiling','acidification','accrual',\n",
    "             'canyoning','caving','clearance',\n",
    "             'closure','decision','diagnosis','distribution','drainage',\n",
    "             'electrification','evaporation', 'failure','formation','hypnosis',\n",
    "             'impact','infiltration','interrogation',\n",
    "             'maintenance','macgyvering','melting','picnicking','proposal',\n",
    "             'proposition','rafting','rebellion','reconciliation','retrieval',\n",
    "             'segmentation','spelunking','swimming']\n",
    "for noun in processes:\n",
    "    get_root_verb( noun )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get noun forms (nominalizations) of a verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_form( verb ):\n",
    "    nouns = list()\n",
    "    for sense in wn.synsets(verb, pos=wn.VERB):\n",
    "        for lemma in sense.lemmas():\n",
    "            name = str(lemma.name())   # remove the \"u\"\n",
    "            # print( [noun, name] )\n",
    "            # if (name[0:3] == noun[0:3]):   ######\n",
    "            if (name == verb):\n",
    "                forms = lemma.derivationally_related_forms()\n",
    "                for form in forms:\n",
    "                    # form is of type Lemma\n",
    "                    fset = form.synset()\n",
    "                    if (fset.pos() == wn.NOUN):\n",
    "                        APPEND = True\n",
    "                        noun = form.name()\n",
    "                        ## nouns.append( noun )\n",
    "                        #--------------------------------\n",
    "                        # Remove \"people\" nouns (CHECK)\n",
    "                        #--------------------------------\n",
    "                        if (noun.endswith(('or','er'))):\n",
    "                            APPEND = False\n",
    "                        #-----------------------------------\n",
    "                        # Don't allow noun == verb (CHECK)\n",
    "                        #-----------------------------------\n",
    "                        if (noun == verb):\n",
    "                            APPEND = False\n",
    "                        if (APPEND):\n",
    "                            nouns.append( noun )\n",
    "    nouns = sorted( set(nouns) )\n",
    "    n_nouns = len(nouns)\n",
    "    if (n_nouns == 0):\n",
    "        print('### Sorry, no noun form found for: ' + verb)\n",
    "    else:\n",
    "        # print( 'n_nouns = ', n_nouns )\n",
    "        dum = 0\n",
    "    for noun in nouns:\n",
    "        print( noun )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abatement\n",
      "### Sorry, no noun form found for: abseil\n",
      "acid\n",
      "acidification\n",
      "accrual\n",
      "accruement\n",
      "### Sorry, no noun form found for: canyon\n",
      "### Sorry, no noun form found for: cave\n",
      "clearance\n",
      "clearing\n",
      "closing\n",
      "closure\n",
      "deciding\n",
      "decision\n",
      "diagnosing\n",
      "diagnosis\n",
      "distribution\n",
      "drainage\n",
      "electricity\n",
      "electrification\n",
      "evaporation\n",
      "failing\n",
      "failure\n",
      "formation\n",
      "hypnosis\n",
      "impaction\n",
      "infiltration\n",
      "interrogation\n",
      "maintenance\n",
      "### Sorry, no noun form found for: macgyver\n",
      "melting\n",
      "### Sorry, no noun form found for: picnic\n",
      "proposal\n",
      "proposition\n",
      "### Sorry, no noun form found for: proposition\n",
      "### Sorry, no noun form found for: raft\n",
      "rebellion\n",
      "reconciliation\n",
      "retrieval\n",
      "segmentation\n",
      "### Sorry, no noun form found for: spelunk\n",
      "swimming\n"
     ]
    }
   ],
   "source": [
    "verbs = ['abate','abseil','acidify','accrue','canyon','cave',\n",
    "         'clear','close','decide','diagnose','distribute','drain',\n",
    "         'electrify','evaporate','fail','form','hypnotize','impact',\n",
    "         'infiltrate','interrogate','maintain','macgyver','melt',\n",
    "         'picnic','propose','proposition','raft','rebel','reconcile',\n",
    "         'retrieve','segment','spelunk','swim']\n",
    "for verb in verbs:\n",
    "    get_noun_form( verb )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a Wikipedia page and parse it with Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def is_downloadable(url):\n",
    "    \"\"\"\n",
    "    Does the url contain a downloadable resource\n",
    "    \"\"\"\n",
    "    h = requests.head(url, allow_redirects=True)\n",
    "    header = h.headers\n",
    "    content_type = header.get('content-type')\n",
    "    if 'text' in content_type.lower():\n",
    "        return False\n",
    "    if 'html' in content_type.lower():\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Reflectance'\n",
    "print(is_downloadable(url))\n",
    "\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "# open('Reflectance.bin', 'wb').write(r.content)\n",
    "# r.content[5000:6000]  # (as bytes)\n",
    "# r.text[5000:6000]\n",
    "# type(r.text)\n",
    "# type(r.json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(r.text, 'lxml')          # (HTML parser)\n",
    "# soup = BeautifulSoup(r.text, 'lxml-xml')    # (XML parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<p><b>Reflectance</b> of the surface of a material is its effectiveness in reflecting <a href=\"/wiki/Radiant_energy\" title=\"Radiant energy\">radiant energy</a>. It is the fraction of incident electromagnetic power that is reflected at an interface. The reflectance spectrum or spectral reflectance curve is the plot of the reflectance as a function of <a href=\"/wiki/Wavelength\" title=\"Wavelength\">wavelength</a>.\\n</p>,\n",
       " <p>The <i>hemispherical reflectance</i> of a surface, denoted <i>R</i>, is defined as<sup class=\"reference\" id=\"cite_ref-ISO_9288-1989_1-0\"><a href=\"#cite_note-ISO_9288-1989-1\">[1]</a></sup>\\n</p>,\n",
       " <p>where\\n</p>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pars = soup.find_all('p')\n",
    "# type(pars)\n",
    "print( len(pars) )\n",
    "pars[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Applications\n",
      "#Directional_reflectance\n",
      "#External_links\n",
      "#Grating_efficiency\n",
      "#Hemispherical_reflectance\n",
      "#Mathematical_definitions\n",
      "#References\n",
      "#Reflectivity\n",
      "#SI_radiometry_units\n",
      "#See_also\n",
      "#Spectral_directional_reflectance\n",
      "#Spectral_hemispherical_reflectance\n",
      "#Surface_type\n",
      "#Water_reflectance\n",
      "#mw-head\n",
      "#p-search\n",
      "Absorbance\n",
      "Absorptance\n",
      "Albedo\n",
      "Aluminium\n",
      "Attenuation_coefficient\n",
      "Bidirectional_reflectance_distribution_function\n",
      "Category:All_articles_with_unsourced_statements\n",
      "Category:Articles_with_unsourced_statements_from_May_2015\n",
      "Category:Dimensionless_numbers\n",
      "Category:Physical_quantities\n",
      "Category:Radiometry\n",
      "Compendium_of_Chemical_Terminology\n",
      "Complex_number\n",
      "Diffraction_efficiency\n",
      "Diffraction_grating\n",
      "Diffuse_reflection\n",
      "Electric_field\n",
      "Emissivity\n",
      "Flux_density\n",
      "Frequency\n",
      "Fresnel_equation\n",
      "Fresnel_equations\n",
      "Fresnel_power_reflection\n",
      "Fresnel_reflection_coefficient\n",
      "Gold\n",
      "Half-space_(geometry)\n",
      "Help:Category\n",
      "Help:Contents\n",
      "Hertz\n",
      "Index_of_refraction\n",
      "International_Commission_on_Illumination\n",
      "International_Organization_for_Standardization\n",
      "International_Standard_Book_Number\n",
      "International_Union_of_Pure_and_Applied_Chemistry\n",
      "Irradiance\n",
      "Jansky\n",
      "Joule\n",
      "Lambda\n",
      "Lambert%27s_cosine_law\n",
      "Lambertian_reflectance\n",
      "Light_Reflectance_Value\n",
      "Main_Page\n",
      "Mirror\n",
      "None\n",
      "Nu_(letter)\n",
      "Omega\n",
      "Optics\n",
      "Photometry_(optics)\n",
      "Photon\n",
      "Physical_quantity\n",
      "Portal:Contents\n",
      "Portal:Current_events\n",
      "Portal:Featured_content\n",
      "Radar\n",
      "Radiance\n",
      "Radiant_energy\n",
      "Radiant_energy_density\n",
      "Radiant_exitance\n",
      "Radiant_exposure\n",
      "Radiant_flux\n",
      "Radiant_intensity\n",
      "Radiometry\n",
      "Radiosity_(radiometry)\n",
      "Real_number\n",
      "Reflectance\n",
      "SI\n",
      "Silver\n",
      "Solar_flux_unit\n",
      "Solar_thermal_energy\n",
      "Special:BookSources/0-8053-8566-5\n",
      "Special:MyContributions\n",
      "Special:MyTalk\n",
      "Special:Random\n",
      "Special:RecentChanges\n",
      "Special:RecentChangesLinked/Reflectance\n",
      "Special:SpecialPages\n",
      "Special:WhatLinksHere/Reflectance\n",
      "Spectral_flux_density\n",
      "Specular_reflection\n",
      "Standards_organization\n",
      "Steradian\n",
      "Sun_path\n",
      "Talk:Reflectance\n",
      "Telecommunication\n",
      "Template:SI_radiometry_units\n",
      "Template_talk:SI_radiometry_units\n",
      "Transmittance\n",
      "Watt\n",
      "Wavelength\n",
      "Waviness\n",
      "http://goldbook.iupac.org/R05235.html\n",
      "http://www.cie.co.at/index.php/index.php?i_ca_id=306\n",
      "http://www.graphics.cornell.edu/online/measurements/reflectance/index.html\n",
      "http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=16943\n"
     ]
    }
   ],
   "source": [
    "links = soup.find_all('a')\n",
    "n = len(links)\n",
    "# print( n )\n",
    "\n",
    "# Use numpy arrays vs. appended list for speed,\n",
    "# using unicode strings up to 100 chars each.\n",
    "import numpy as np\n",
    "alist = np.zeros(n, dtype='U100')\n",
    "\n",
    "# Extract all the URLs on the page and save in a\n",
    "# string array, but remove ones we don't want\n",
    "for k in range(n):\n",
    "    s = str(links[k].get('href'))\n",
    "    if (s.startswith( ('/w/','#cite_','//','https','/wiki/Wikipedia:') )):\n",
    "        dum = 0\n",
    "    elif (s.endswith( ('.png','.jpg','.svg'))):\n",
    "        dum = 0\n",
    "    elif (s.startswith('/wiki/')):\n",
    "        alist[k] = s[6:]\n",
    "    elif (s.startswith('File:')):\n",
    "        alist[k] = s[5:]\n",
    "    else:\n",
    "        alist[k] = s\n",
    "\n",
    "# Print just the unique, sorted \"link strings\"\n",
    "blist = sorted(set( alist ))\n",
    "for item in blist:\n",
    "    print( item )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(soup.prettify())\n",
    "# soup.title()\n",
    "# print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Indra and run a test with the Eidos reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named indra",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-43a5799d8ce0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mindra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mindra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msources\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meidos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentence1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Conflict causes displacement, which leads to hunger.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msentence2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Water trucking has decreased due to the cost of fuel.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meidos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0msentence1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named indra"
     ]
    }
   ],
   "source": [
    "import indra\n",
    "from indra.sources import eidos\n",
    "sentence1 = \"Conflict causes displacement, which leads to hunger.\"\n",
    "sentence2 = \"Water trucking has decreased due to the cost of fuel.\"\n",
    "ep = eidos.process_text( sentence1 )\n",
    "ep.statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named indra.assemblers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-63183a94825e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mindra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massemblers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCAGAssembler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJavascript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCAGAssembler\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatements\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named indra.assemblers"
     ]
    }
   ],
   "source": [
    "from indra.assemblers import CAGAssembler\n",
    "from IPython.core.display import Javascript\n",
    "\n",
    "ca = CAGAssembler( ep.statements )\n",
    "ca.make_model()\n",
    "Javascript( ca.generate_jupyter_js() )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes Python version 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import several Python packages to check availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib\n",
    "import scipy\n",
    "import bokeh\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These were added explicitly to environment.yml for binder.\n",
    "import autocorrect\n",
    "import plotly\n",
    "# import pyenchant\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download some data to use with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  The nltk.download() function with no argument launches a dialog to choose data such as corpora, but you can also provide an argument to specify what to download. See: https://stackoverflow.com/questions/5843817/programmatically-install-nltk-corpora-models-i-e-without-the-gui-downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('verbnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some tests with WordNet from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boil.n.01\n",
      "boiling_point.n.01\n",
      "boil.v.01\n",
      "boil.v.02\n",
      "boil.v.03\n",
      "churn.v.02\n",
      "seethe.v.02\n",
      " \n",
      "jump.n.01\n",
      "leap.n.02\n",
      "jump.n.03\n",
      "startle.n.01\n",
      "jump.n.05\n",
      "jump.n.06\n",
      "jump.v.01\n",
      "startle.v.02\n",
      "jump.v.03\n",
      "jump.v.04\n",
      "leap_out.v.01\n",
      "jump.v.06\n",
      "rise.v.11\n",
      "jump.v.08\n",
      "derail.v.02\n",
      "chute.v.01\n",
      "jump.v.11\n",
      "jumpstart.v.01\n",
      "jump.v.13\n",
      "leap.v.02\n",
      "alternate.v.01\n",
      " \n",
      "flow.n.01\n",
      "flow.n.02\n",
      "flow.n.03\n",
      "flow.n.04\n",
      "stream.n.04\n",
      "stream.n.02\n",
      "menstruation.n.01\n",
      "flow.v.01\n",
      "run.v.06\n",
      "flow.v.03\n",
      "flow.v.04\n",
      "hang.v.05\n",
      "flow.v.06\n",
      "menstruate.v.01\n",
      " \n",
      "gyrate.v.01\n",
      "spin.v.01\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "sets = wn.synsets('boil')\n",
    "for s in sets:\n",
    "    print( s.name() )\n",
    "    # print( s.pos() )\n",
    "\n",
    "print(' ')\n",
    "sets = wn.synsets('jump')\n",
    "for s in sets:\n",
    "    print( s.name() )\n",
    "    # print( s.pos() )\n",
    "\n",
    "print(' ')\n",
    "sets = wn.synsets('flow')\n",
    "for s in sets:\n",
    "    print( s.name() )\n",
    "    # print( s.pos() )\n",
    "    \n",
    "print(' ')\n",
    "sets = wn.synsets('gyrate')\n",
    "for s in sets:\n",
    "    print( s.name() )\n",
    "    # print( s.pos() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some tests with VerbNet from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'December',\n",
       " u'FedEx',\n",
       " u'UPS',\n",
       " u'abandon',\n",
       " u'abase',\n",
       " u'abash',\n",
       " u'abate',\n",
       " u'abbreviate',\n",
       " u'abduct',\n",
       " u'abet',\n",
       " u'abhor',\n",
       " u'abolish',\n",
       " u'abound',\n",
       " u'abrade',\n",
       " u'abridge',\n",
       " u'absolve',\n",
       " u'abstain',\n",
       " u'abstract',\n",
       " u'abuse',\n",
       " u'abut',\n",
       " u'accelerate',\n",
       " u'accept',\n",
       " u'acclaim',\n",
       " u'accompany',\n",
       " u'accrue']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import verbnet as vn\n",
    "vn.lemmas()[0:25]\n",
    "\n",
    "# help(vn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment for generating nominalizations of verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GSN process names are nominalizations of verbs.  One idea for how to automatically generate a list of these process names from a list of verbs (e.g. from VerbNet) is to concatenate some of the 11 or so possible verb nominalization endings directly to the verb, then apply autocorrect and check if result is a noun.  Standard endings are:\n",
    "**tion** (absorption, convection),\n",
    "**sion** (conversion, dispersion),\n",
    "**cion** (suspicion, coercion),\n",
    "**ing** (swimming, upwelling),\n",
    "**age** (drainage, seepage),\n",
    "**y** (discovery, recovery),\n",
    "**al** (arrival, retrieval),\n",
    "**ance** (acceptance, attendance),\n",
    "**ence** (existence, maintanence)\n",
    "**ment** (alignment, improvement),\n",
    "**ure** (failure, departure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['proposition', 'proposal', 'proposeence']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autocorrect import spell\n",
    "\n",
    "[spell('proposetion'), spell('proposeal'), spell('proposeence')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that **spell** may return strings that aren't words with no change, like the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['distraction', u'distracted', 'distractence']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[spell('distracttion'),spell('distractal'),spell('distractence')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
